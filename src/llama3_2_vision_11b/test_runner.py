#!/usr/bin/env python3
"""
Ejecutor de tests para modelos de visi√≥n.

Este m√≥dulo proporciona la l√≥gica principal para ejecutar tests
de rendimiento en modelos multimodales de forma estructurada.
"""

import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Union

from .ollama_client import OllamaClient
from .image_utils import ImageProcessor, safe_encode_image
from .metrics import MetricsAnalyzer


class VisionTestRunner:
    """Ejecutor de tests para modelos de visi√≥n."""
    
    def __init__(self, host: str = "http://localhost:11434", 
                 timeout: int = 600):
        """
        Inicializa el ejecutor de tests.
        
        Args:
            host: URL del servidor Ollama
            timeout: Timeout por defecto
        """
        self.client = OllamaClient(host, timeout)
        self.image_processor = ImageProcessor()
        self.metrics_analyzer = MetricsAnalyzer()
        self.host = host
    
    def verify_setup(self) -> bool:
        """
        Verifica que el setup est√© listo para ejecutar tests.
        
        Returns:
            bool: True si todo est√° listo
        """
        if not self.client.is_server_available():
            print(f"‚ùå Error: No se puede conectar con {self.host}", file=sys.stderr)
            print("   Ejecuta: sudo systemctl start ollama", file=sys.stderr)
            return False
        
        print(f"‚úÖ Servidor Ollama disponible en {self.host}")
        return True
    
    def run_warmup(self, model: str) -> bool:
        """
        Ejecuta warmup del modelo.
        
        Args:
            model: Nombre del modelo
            
        Returns:
            bool: True si el warmup fue exitoso
        """
        print("üî• Ejecutando warmup...")
        
        if self.client.warmup(model):
            print("‚úÖ Warmup completado\n")
            return True
        else:
            print("‚ö†Ô∏è  Warning: Warmup fall√≥ (continuamos)\n", file=sys.stderr)
            return False
    
    def run_text_only_test(self, 
                          model: str,
                          prompt: str,
                          options: Dict[str, Any],
                          runs: int = 3,
                          stream: bool = False) -> List[Dict[str, Any]]:
        """
        Ejecuta test solo con texto.
        
        Args:
            model: Nombre del modelo
            prompt: Texto de entrada
            options: Opciones de generaci√≥n
            runs: N√∫mero de repeticiones
            stream: Si hacer streaming
            
        Returns:
            Lista de m√©tricas por ejecuci√≥n
        """
        print("\nüìù === TEST MODO TEXTO ===")
        print(f"Prompt: {prompt[:80]}...")
        
        metrics_list = []
        
        for run in range(1, runs + 1):
            print(f"\nüîÑ Run {run}/{runs} (texto)")
            
            try:
                response, stats, elapsed = self.client.generate_response(
                    model, prompt, None, options, stream
                )
                
                if not stream and response:
                    print(f"üí¨ Respuesta: {response[:150]}...")
                
                # Procesar m√©tricas
                metrics = self.metrics_analyzer.process_ollama_stats(stats, elapsed)
                metrics.update({"mode": "text", "run": run})
                metrics_list.append(metrics)
                
                # Mostrar estad√≠sticas
                self._print_run_stats(metrics)
                self.metrics_analyzer.add_metrics(metrics)
                
            except Exception as e:
                print(f"‚ùå Error en run {run}: {e}", file=sys.stderr)
                continue
        
        return metrics_list
    
    def run_vision_test(self,
                       model: str, 
                       prompt: str,
                       image_path: Union[str, Path],
                       options: Dict[str, Any],
                       runs: int = 3,
                       stream: bool = False) -> List[Dict[str, Any]]:
        """
        Ejecuta test con imagen y texto.
        
        Args:
            model: Nombre del modelo
            prompt: Texto de entrada
            image_path: Ruta a la imagen
            options: Opciones de generaci√≥n
            runs: N√∫mero de repeticiones
            stream: Si hacer streaming
            
        Returns:
            Lista de m√©tricas por ejecuci√≥n
        """
        print("\nüñºÔ∏è  === TEST MODO VISI√ìN ===")
        print(f"Imagen: {image_path}")
        print(f"Prompt: {prompt[:80]}...")
        
        # Validar y codificar imagen
        if not self.image_processor.validate_image_path(image_path):
            print(f"‚ùå Error: Imagen no v√°lida: {image_path}", file=sys.stderr)
            return []
        
        image_base64 = safe_encode_image(image_path)
        if not image_base64:
            print("‚ùå Error: No se pudo codificar la imagen", file=sys.stderr)
            return []
        
        # Mostrar info de imagen
        image_info = self.image_processor.get_image_info(image_path)
        if "error" not in image_info:
            print(f"üìä Imagen: {image_info['name']} ({image_info['size_mb']} MB)")
        
        metrics_list = []
        
        for run in range(1, runs + 1):
            print(f"\nüîÑ Run {run}/{runs} (visi√≥n)")
            
            try:
                response, stats, elapsed = self.client.generate_response(
                    model, prompt, [image_base64], options, stream
                )
                
                if not stream and response:
                    print(f"üí¨ Respuesta: {response[:150]}...")
                
                # Procesar m√©tricas
                metrics = self.metrics_analyzer.process_ollama_stats(stats, elapsed)
                metrics.update({
                    "mode": "vision", 
                    "run": run,
                    "image_path": str(image_path)
                })
                metrics_list.append(metrics)
                
                # Mostrar estad√≠sticas
                self._print_run_stats(metrics)
                print("   ‚ìò  Procesamiento de imagen incluido en prefill")
                self.metrics_analyzer.add_metrics(metrics)
                
            except Exception as e:
                print(f"‚ùå Error en run {run}: {e}", file=sys.stderr)
                continue
        
        return metrics_list
    
    def run_comparison_test(self,
                           model: str,
                           text_prompt: str,
                           vision_prompt: str,
                           image_path: Union[str, Path],
                           options: Dict[str, Any],
                           runs: int = 3,
                           stream: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        Ejecuta test comparativo entre modo texto y visi√≥n.
        
        Args:
            model: Nombre del modelo
            text_prompt: Prompt para modo texto
            vision_prompt: Prompt para modo visi√≥n
            image_path: Ruta a la imagen
            options: Opciones de generaci√≥n
            runs: N√∫mero de repeticiones por modo
            stream: Si hacer streaming
            
        Returns:
            Dict con m√©tricas de ambos modos
        """
        results = {
            "text_metrics": [],
            "vision_metrics": []
        }
        
        # Test modo texto
        results["text_metrics"] = self.run_text_only_test(
            model, text_prompt, options, runs, stream
        )
        
        # Test modo visi√≥n
        results["vision_metrics"] = self.run_vision_test(
            model, vision_prompt, image_path, options, runs, stream
        )
        
        # Mostrar comparaci√≥n
        if results["text_metrics"] and results["vision_metrics"]:
            self._print_comparison(results["text_metrics"], results["vision_metrics"])
        
        return results
    
    def _print_run_stats(self, metrics: Dict[str, Any]) -> None:
        """
        Imprime estad√≠sticas de un run.
        
        Args:
            metrics: M√©tricas del run
        """
        prefill_tps = metrics.get('prefill_tps')
        decode_tps = metrics.get('decode_tps')
        
        prefill_str = f"{prefill_tps:.1f}" if prefill_tps else "n/a"
        decode_str = f"{decode_tps:.1f}" if decode_tps else "n/a"
        
        print(f"   üìä wall={metrics['wall_time_s']:.2f}s | "
              f"prefill={metrics.get('prefill_tokens', 0)} tok @ {prefill_str} t/s | "
              f"decode={metrics.get('decode_tokens', 0)} tok @ {decode_str} t/s")
    
    def _print_comparison(self, text_metrics: List[Dict[str, Any]], 
                         vision_metrics: List[Dict[str, Any]]) -> None:
        """
        Imprime comparaci√≥n entre modos.
        
        Args:
            text_metrics: M√©tricas del modo texto
            vision_metrics: M√©tricas del modo visi√≥n
        """
        print("\n" + "="*70)
        print("üìä COMPARACI√ìN TEXTO vs VISI√ìN")
        print("="*70)
        
        # Obtener estad√≠sticas resumidas
        text_summary = self.metrics_analyzer.get_summary_stats("text")
        vision_summary = self.metrics_analyzer.get_summary_stats("vision")
        
        if "error" not in text_summary:
            print(f"\nüìù MODO TEXTO:")
            print(f"   Velocidad promedio: {text_summary['decode_tps']['mean']:.1f} t/s")
            print(f"   Tiempo promedio: {text_summary['wall_time_s']['mean']:.2f} s")
        
        if "error" not in vision_summary:
            print(f"\nüñºÔ∏è  MODO VISI√ìN:")
            print(f"   Velocidad promedio: {vision_summary['decode_tps']['mean']:.1f} t/s")
            print(f"   Tiempo promedio: {vision_summary['wall_time_s']['mean']:.2f} s")
            
            # Calcular overhead
            if "error" not in text_summary:
                overhead = (vision_summary['wall_time_s']['mean'] - 
                           text_summary['wall_time_s']['mean'])
                ratio = (text_summary['decode_tps']['mean'] / 
                        vision_summary['decode_tps']['mean'])
                
                print(f"   Overhead por imagen: ~{overhead:.1f} s")
                print(f"\n‚ö° Factor velocidad (texto/visi√≥n): {ratio:.2f}x")
    
    def save_results(self, filepath: str, 
                    additional_fields: Optional[Dict[str, Any]] = None) -> bool:
        """
        Guarda resultados en archivo JSONL.
        
        Args:
            filepath: Ruta del archivo
            additional_fields: Campos adicionales
            
        Returns:
            bool: True si se guard√≥ correctamente
        """
        return self.metrics_analyzer.save_to_jsonl(filepath, additional_fields)
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Obtiene resumen de todas las m√©tricas.
        
        Returns:
            Dict con resumen completo
        """
        return {
            "overall": self.metrics_analyzer.get_summary_stats(),
            "text_mode": self.metrics_analyzer.get_summary_stats("text"),
            "vision_mode": self.metrics_analyzer.get_summary_stats("vision")
        }
