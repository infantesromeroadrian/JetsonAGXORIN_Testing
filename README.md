# ğŸš€ Testing y Benchmarking de NVIDIA Jetson AGX Orin para LLMs

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto tiene como objetivo evaluar y documentar el rendimiento del **NVIDIA Jetson AGX Orin Developer Kit** ejecutando modelos de lenguaje grandes (LLMs), especÃ­ficamente utilizando **Ollama** como servidor de inferencia. El proyecto incluye herramientas de benchmarking, anÃ¡lisis comparativo con otras GPUs, y documentaciÃ³n tÃ©cnica detallada sobre las capacidades del hardware.

### ğŸ¯ Objetivos Principales

1. **EvaluaciÃ³n de Rendimiento**: Medir la velocidad de inferencia (tokens/segundo) de diferentes modelos LLM en el Jetson AGX Orin
2. **AnÃ¡lisis Comparativo**: Comparar el rendimiento con otras soluciones de GPU (RTX Ada 2000)
3. **OptimizaciÃ³n de Recursos**: Identificar las mejores prÃ¡cticas para ejecutar LLMs en arquitectura ARM con memoria unificada
4. **DocumentaciÃ³n TÃ©cnica**: Proporcionar guÃ­as detalladas sobre configuraciÃ³n y uso del Jetson para IA

## ğŸ—ï¸ Arquitectura del Proyecto

```
Testin_Jetson_AGX_ORIN/
â”‚
â”œâ”€â”€ ğŸ“ src/                       # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ test_ollama_llama3_2_3b.py    # Script de testing individual
â”‚   â””â”€â”€ sweep_ollama_llama3_2_3b.py   # Script de barrido paramÃ©trico
â”‚
â”œâ”€â”€ ğŸ“ docs/                      # DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ Informe_Tecnico_Jetson_AGX_Orin.md  # Informe tÃ©cnico completo
â”‚   â””â”€â”€ Testing_JetsonAGXORIN.docx          # DocumentaciÃ³n adicional
â”‚
â”œâ”€â”€ ğŸ“ data/                      # Datos de pruebas y resultados
â”œâ”€â”€ ğŸ“ models/                    # Modelos y configuraciones
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks para anÃ¡lisis
â”œâ”€â”€ ğŸ“ assets/                    # Recursos multimedia
â”‚
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â””â”€â”€ testing_jetson_venv/         # Entorno virtual Python
```

## ğŸ’» Especificaciones del Hardware

### NVIDIA Jetson AGX Orin Developer Kit

| CaracterÃ­stica | EspecificaciÃ³n |
|----------------|----------------|
| **Arquitectura** | ARM aarch64 (64-bit) |
| **GPU** | NVIDIA integrada (compartida UMA) |
| **RAM** | 64 GB LPDDR5 (61 GB utilizable) |
| **Memoria GPU** | Unificada (UMA) - comparte RAM del sistema |
| **Almacenamiento** | eMMC ~59.2 GB |
| **CUDA** | 12.6 |
| **JetPack** | 6.2.1 (L4T r36.4.4) |
| **Sistema Operativo** | Ubuntu 22.04 LTS (Jammy) |

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

1. **Jetson AGX Orin** con JetPack 6.2.1 o superior instalado
2. **Python 3.8+** instalado en el sistema
3. **Ollama** servidor de inferencia instalado y configurado

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd Testin_Jetson_AGX_ORIN

# 2. Crear y activar entorno virtual
python3 -m venv testing_jetson_venv
source testing_jetson_venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Verificar instalaciÃ³n de Ollama
sudo systemctl status ollama

# 5. Descargar modelo de prueba
ollama pull llama3.2:3b
```

## ğŸš€ Uso

### Test Simple de Rendimiento

```bash
# Ejecutar test bÃ¡sico con 5 repeticiones
python src/test_ollama_llama3_2_3b.py --model llama3.2:3b -n 5 --out metrics.jsonl

# Test con streaming (ver tokens en tiempo real)
python src/test_ollama_llama3_2_3b.py --model llama3.2:3b --stream
```

### Barrido ParamÃ©trico Completo

```bash
# Ejecutar barrido con mÃºltiples configuraciones
python src/sweep_ollama_llama3_2_3b.py \
    --model llama3.2:3b \
    --ctx 2048,4096 \
    --num-predict 128,256,512 \
    --temp 0,0.4,0.7 \
    --runs 3 \
    --csv results.csv \
    --out metrics.jsonl
```

### ParÃ¡metros Disponibles

#### `test_ollama_llama3_2_3b.py`
- `--host`: URL del servidor Ollama (default: http://localhost:11434)
- `--model`: Modelo a evaluar (default: llama3.2:3b)
- `--prompt`: Texto de entrada para la prueba
- `-n, --runs`: NÃºmero de repeticiones
- `--stream`: Mostrar salida token por token
- `--ctx`: TamaÃ±o de ventana de contexto
- `--temp`: Temperatura de generaciÃ³n
- `--out`: Archivo de salida para mÃ©tricas (JSONL)

#### `sweep_ollama_llama3_2_3b.py`
- Todos los parÃ¡metros anteriores, mÃ¡s:
- `--cycles`: Repetir el barrido completo N veces
- `--num-predict`: Lista de tokens a generar
- `--seed`: Semillas para reproducibilidad
- `--csv`: Archivo CSV de salida
- `--warmup`: Hacer calentamiento antes de cada prueba
- `--sleep`: Pausa entre ejecuciones

## ğŸ“Š Resultados y MÃ©tricas

### Rendimiento en Jetson AGX Orin

| MÃ©trica | Valor |
|---------|-------|
| **Modelo** | llama3.2:3b (~2.0 GB) |
| **Velocidad de DecodificaciÃ³n** | ~44.8 tokens/seg |
| **Uso de GPU** | 90-99% @ 1.3 GHz |
| **Temperatura** | 60-61Â°C (estable) |
| **RAM Utilizada** | ~5.6 GB de 61 GB |
| **Consumo EnergÃ©tico** | 31-35W en carga, 5W en reposo |

### ComparaciÃ³n con RTX Ada 2000

| Hardware | Velocidad (t/s) | Factor de AceleraciÃ³n |
|----------|-----------------|----------------------|
| RTX Ada 2000 | 74.65 | 1.67Ã— |
| Jetson AGX Orin | 44.80 | 1.00Ã— (referencia) |

## ğŸ“ˆ Monitoreo del Sistema

### Comandos Ãštiles

```bash
# Monitorear uso de recursos en tiempo real
sudo tegrastats --interval 1000

# Ver memoria disponible
free -h

# Verificar almacenamiento
df -h

# Monitorear temperatura y frecuencias
watch -n 1 'cat /sys/devices/virtual/thermal/thermal_zone*/temp'
```

## ğŸ¯ Casos de Uso Recomendados

### âœ… Ideal para:
- **Modelos de 4-8B parÃ¡metros** en INT4/FP8
- Aplicaciones de edge computing con IA
- Inferencia en tiempo real con restricciones de energÃ­a
- Desarrollo y prototipado de soluciones embebidas de IA

### âš ï¸ Considerar con cuidado:
- **Modelos de 13B parÃ¡metros** (justo en el lÃ­mite de RAM)
- Aplicaciones con contextos muy largos (>8K tokens)

### âŒ No recomendado:
- Modelos superiores a 13B parÃ¡metros
- Entrenamiento de modelos grandes
- Aplicaciones que requieran mÃºltiples modelos simultÃ¡neos

## ğŸ”¬ AnÃ¡lisis TÃ©cnico

### Arquitectura de Memoria Unificada (UMA)

El Jetson AGX Orin utiliza una arquitectura de memoria unificada donde CPU y GPU comparten la misma RAM del sistema. Esto tiene implicaciones importantes:

**Ventajas:**
- No hay overhead de transferencia CPUâ†”GPU
- GestiÃ³n simplificada de memoria
- Ideal para cargas de trabajo mixtas

**Consideraciones:**
- La "VRAM" disponible = RAM libre del sistema
- Competencia por ancho de banda entre CPU y GPU
- Importante mantener margen de RAM libre

### Optimizaciones Recomendadas

1. **GestiÃ³n de Memoria**
   - Mantener al menos 10-15% de RAM libre
   - Evitar que el sistema use swap
   - Monitorear con `tegrastats` durante inferencia

2. **ConfiguraciÃ³n de Modelos**
   - Usar cuantizaciÃ³n (INT4/INT8) cuando sea posible
   - Ajustar `num_ctx` segÃºn necesidades reales
   - Considerar modelos mÃ¡s pequeÃ±os para producciÃ³n

3. **Rendimiento**
   - Hacer warmup antes de mediciones crÃ­ticas
   - Usar batch processing cuando sea aplicable
   - Configurar power mode en MAXN para mÃ¡ximo rendimiento

## ğŸ“š DocumentaciÃ³n Adicional

- [Informe TÃ©cnico Completo](docs/Informe_Tecnico_Jetson_AGX_Orin.md) - AnÃ¡lisis detallado del hardware y pruebas
- [NVIDIA Jetson Documentation](https://docs.nvidia.com/jetson/)
- [Ollama Documentation](https://ollama.ai/docs)

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ diseÃ±ado con fines educativos y de investigaciÃ³n. Consulte el archivo LICENSE para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- NVIDIA por el desarrollo del Jetson AGX Orin
- Comunidad de Ollama por el servidor de inferencia
- Contribuidores y testers del proyecto

## ğŸ“§ Contacto

Para preguntas, sugerencias o colaboraciones, por favor abra un issue en el repositorio.

---

**Nota**: Este proyecto estÃ¡ en desarrollo activo. Las mÃ©tricas y recomendaciones pueden actualizarse segÃºn nuevos hallazgos y optimizaciones.
