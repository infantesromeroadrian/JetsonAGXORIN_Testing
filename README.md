# ğŸš€ Testing y Benchmarking de NVIDIA Jetson AGX Orin para LLMs

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto tiene como objetivo evaluar y documentar el rendimiento del **NVIDIA Jetson AGX Orin Developer Kit** ejecutando modelos de lenguaje grandes (LLMs), especÃ­ficamente utilizando **Ollama** como servidor de inferencia. El proyecto incluye herramientas de benchmarking, anÃ¡lisis comparativo con otras GPUs, y documentaciÃ³n tÃ©cnica detallada sobre las capacidades del hardware.

### ğŸ¯ Objetivos Principales

1. **EvaluaciÃ³n de Rendimiento**: Medir la velocidad de inferencia (tokens/segundo) de diferentes modelos LLM en el Jetson AGX Orin
2. **AnÃ¡lisis Comparativo**: Comparar el rendimiento con otras soluciones de GPU (RTX Ada 2000)
3. **OptimizaciÃ³n de Recursos**: Identificar las mejores prÃ¡cticas para ejecutar LLMs en arquitectura ARM con memoria unificada
4. **DocumentaciÃ³n TÃ©cnica**: Proporcionar guÃ­as detalladas sobre configuraciÃ³n y uso del Jetson para IA

## ğŸ—ï¸ Arquitectura del Proyecto

```
Testin_Jetson_AGX_ORIN/
â”‚
â”œâ”€â”€ ğŸ“ src/                       # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ test_ollama_llama3_2_3b.py         # Testing individual modelo 3B
â”‚   â”œâ”€â”€ sweep_ollama_llama3_2_3b.py        # Barrido paramÃ©trico modelo 3B
â”‚   â”œâ”€â”€ test_ollama_llama3_2_vision_11b.py # Testing modelo visiÃ³n 11B
â”‚   â””â”€â”€ sweep_ollama_llama3_2_vision_11b.py # Barrido modelo visiÃ³n 11B
â”‚
â”œâ”€â”€ ğŸ“ docs/                      # DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ Informe_Tecnico_Jetson_AGX_Orin.md  # Informe tÃ©cnico completo
â”‚   â””â”€â”€ Testing_JetsonAGXORIN.docx          # DocumentaciÃ³n adicional
â”‚
â”œâ”€â”€ ğŸ“ data/                      # Datos de pruebas y resultados
â”œâ”€â”€ ğŸ“ models/                    # Modelos y configuraciones
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks para anÃ¡lisis
â”œâ”€â”€ ğŸ“ assets/                    # Recursos multimedia
â”‚
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â””â”€â”€ testing_jetson_venv/         # Entorno virtual Python
```

## ğŸ’» Especificaciones del Hardware

### NVIDIA Jetson AGX Orin Developer Kit

| CaracterÃ­stica | EspecificaciÃ³n |
|----------------|----------------|
| **Arquitectura** | ARM aarch64 (64-bit) |
| **GPU** | NVIDIA integrada (compartida UMA) |
| **RAM** | 64 GB LPDDR5 (61 GB utilizable) |
| **Memoria GPU** | Unificada (UMA) - comparte RAM del sistema |
| **Almacenamiento** | eMMC ~59.2 GB |
| **CUDA** | 12.6 |
| **JetPack** | 6.2.1 (L4T r36.4.4) |
| **Sistema Operativo** | Ubuntu 22.04 LTS (Jammy) |

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

1. **Jetson AGX Orin** con JetPack 6.2.1 o superior instalado
2. **Python 3.8+** instalado en el sistema
3. **Ollama** servidor de inferencia instalado y configurado

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd Testin_Jetson_AGX_ORIN

# 2. Crear y activar entorno virtual
python3 -m venv testing_jetson_venv
source testing_jetson_venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Verificar instalaciÃ³n de Ollama
sudo systemctl status ollama

# 5. Descargar modelo de prueba
ollama pull llama3.2:3b
```

## ğŸš€ Uso

### Test Simple de Rendimiento (Modelo 3B)

```bash
# Ejecutar test bÃ¡sico con 5 repeticiones
python src/test_ollama_llama3_2_3b.py --model llama3.2:3b -n 5 --out metrics.jsonl

# Test con streaming (ver tokens en tiempo real)
python src/test_ollama_llama3_2_3b.py --model llama3.2:3b --stream
```

### Test con Modelo de VisiÃ³n (11B)

```bash
# Test con imagen (modo visiÃ³n)
python src/test_ollama_llama3_2_vision_11b.py --image assets/puerto-new-york-1068x570.webp -n 3

# ComparaciÃ³n texto vs visiÃ³n
python src/test_ollama_llama3_2_vision_11b.py --image assets/puerto-new-york-1068x570.webp --test-mode both

# Solo modo texto (sin usar capacidades de visiÃ³n)
python src/test_ollama_llama3_2_vision_11b.py --test-mode text -n 5
```

### Barrido ParamÃ©trico Completo

```bash
# Barrido modelo 3B
python src/sweep_ollama_llama3_2_3b.py \
    --model llama3.2:3b \
    --ctx 2048,4096 \
    --num-predict 128,256,512 \
    --temp 0,0.4,0.7 \
    --runs 3 \
    --csv results.csv \
    --out metrics.jsonl

# Barrido modelo visiÃ³n 11B
python src/sweep_ollama_llama3_2_vision_11b.py \
    --image assets/puerto-new-york-1068x570.webp \
    --ctx 4096,8192 \
    --num-predict 128,256 \
    --runs 3 \
    --csv vision_results.csv
```

### ParÃ¡metros Disponibles

#### `test_ollama_llama3_2_3b.py`
- `--host`: URL del servidor Ollama (default: http://localhost:11434)
- `--model`: Modelo a evaluar (default: llama3.2:3b)
- `--prompt`: Texto de entrada para la prueba
- `-n, --runs`: NÃºmero de repeticiones
- `--stream`: Mostrar salida token por token
- `--ctx`: TamaÃ±o de ventana de contexto
- `--temp`: Temperatura de generaciÃ³n
- `--out`: Archivo de salida para mÃ©tricas (JSONL)

#### `sweep_ollama_llama3_2_3b.py`
- Todos los parÃ¡metros anteriores, mÃ¡s:
- `--cycles`: Repetir el barrido completo N veces
- `--num-predict`: Lista de tokens a generar
- `--seed`: Semillas para reproducibilidad
- `--csv`: Archivo CSV de salida
- `--warmup`: Hacer calentamiento antes de cada prueba
- `--sleep`: Pausa entre ejecuciones

## ğŸ“Š Resultados y MÃ©tricas

### Rendimiento en Jetson AGX Orin

| Modelo | TamaÃ±o | Modo | Velocidad (t/s) | RAM Usada | GPU | Estado |
|--------|--------|------|-----------------|-----------|-----|--------|
| **llama3.2:3b** | ~2 GB | Texto | 44.8 | ~5.6 GB | 90-99% | âœ… Verificado |
| **llama3.2-vision:11b** | ~7 GB | Texto | 25.4 | ~12 GB | 90-99% | âœ… Verificado |
| **llama3.2-vision:11b** | ~7 GB | VisiÃ³n | 13.8 | ~15 GB | 90-99% | âœ… Verificado |

### ComparaciÃ³n con RTX Ada 2000 (modelo 3B)

| Hardware | Velocidad (t/s) | Factor de AceleraciÃ³n |
|----------|-----------------|----------------------|
| RTX Ada 2000 | 74.65 | 1.67Ã— |
| Jetson AGX Orin | 44.80 | 1.00Ã— (referencia) |

### CaracterÃ­sticas del Modelo de VisiÃ³n (llama3.2-vision:11b)

| MÃ©trica | Valor | Estado |
|---------|-------|--------|
| **Velocidad modo texto** | 25.4 t/s | âœ… Verificado |
| **Velocidad modo visiÃ³n** | 13.8 t/s | âœ… Verificado |
| **Factor texto/visiÃ³n** | 1.84Ã— (texto 84% mÃ¡s rÃ¡pido) | âœ… Consistente |
| **Overhead por imagen** | ~15 segundos | âœ… Medido |
| **Prefill texto** | 419-774 tokens/seg | âœ… Muy eficiente |
| **Prefill primera imagen** | 4.1 tokens/seg | âš ï¸ Lento inicial |
| **Prefill imagen en cachÃ©** | 154-161 tokens/seg | âœ… Mucho mejor |
| **Factor vs modelo 3B (texto)** | 57% de velocidad | âœ… Mejor de lo esperado |
| **Factor vs modelo 3B (visiÃ³n)** | 31% de velocidad | âœ… Aceptable para visiÃ³n |
| **Temperatura** | 60-65Â°C (estable) | âœ… Normal |
| **Consumo EnergÃ©tico** | 31-35W en carga, 5W en reposo | âœ… Eficiente |

## ğŸ“ˆ Monitoreo del Sistema

### Comandos Ãštiles

```bash
# Monitorear uso de recursos en tiempo real
sudo tegrastats --interval 1000

# Ver memoria disponible
free -h

# Verificar almacenamiento
df -h

# Monitorear temperatura y frecuencias
watch -n 1 'cat /sys/devices/virtual/thermal/thermal_zone*/temp'
```

## ğŸ¯ Casos de Uso Recomendados

### âœ… Ideal para:
- **Modelos de 1-3B parÃ¡metros** para mÃ¡ximo rendimiento (40-45 t/s)
- **Modelos de 4-8B parÃ¡metros** en INT4/FP8 con buen balance
- **Modelo de visiÃ³n 11B** para anÃ¡lisis de imÃ¡genes (16-22 t/s)
- Aplicaciones de edge computing con IA
- Inferencia en tiempo real con restricciones de energÃ­a
- AnÃ¡lisis de imÃ¡genes local sin cloud (seguridad/privacidad)
- Desarrollo y prototipado de soluciones embebidas de IA

### âš ï¸ Considerar con cuidado:
- **Modelos de 13B parÃ¡metros** (justo en el lÃ­mite de RAM)
- Aplicaciones con contextos muy largos (>8K tokens)
- Procesamiento de mÃºltiples imÃ¡genes en paralelo (overhead de visiÃ³n)
- Primera inferencia con imÃ¡genes nuevas (latencia inicial alta)

### âŒ No recomendado:
- Modelos superiores a 13B parÃ¡metros
- Modelo llama3.2-vision:90b (excede memoria disponible)
- Entrenamiento de modelos grandes
- Aplicaciones que requieran mÃºltiples modelos simultÃ¡neos
- Procesamiento de video en tiempo real con modelos grandes

## ğŸ”¬ AnÃ¡lisis TÃ©cnico

### Arquitectura de Memoria Unificada (UMA)

El Jetson AGX Orin utiliza una arquitectura de memoria unificada donde CPU y GPU comparten la misma RAM del sistema. Esto tiene implicaciones importantes:

**Ventajas:**
- No hay overhead de transferencia CPUâ†”GPU
- GestiÃ³n simplificada de memoria
- Ideal para cargas de trabajo mixtas

**Consideraciones:**
- La "VRAM" disponible = RAM libre del sistema
- Competencia por ancho de banda entre CPU y GPU
- Importante mantener margen de RAM libre

### Optimizaciones Recomendadas

1. **GestiÃ³n de Memoria**
   - Mantener al menos 10-15% de RAM libre
   - Evitar que el sistema use swap
   - Monitorear con `tegrastats` durante inferencia

2. **ConfiguraciÃ³n de Modelos**
   - Usar cuantizaciÃ³n (INT4/INT8) cuando sea posible
   - Ajustar `num_ctx` segÃºn necesidades reales
   - Considerar modelos mÃ¡s pequeÃ±os para producciÃ³n

3. **Rendimiento**
   - Hacer warmup antes de mediciones crÃ­ticas
   - Usar batch processing cuando sea aplicable
   - Configurar power mode en MAXN para mÃ¡ximo rendimiento

## ğŸ“š DocumentaciÃ³n Adicional

- [Informe TÃ©cnico Completo](docs/Informe_Tecnico_Jetson_AGX_Orin.md) - AnÃ¡lisis detallado del hardware, pruebas de modelos 3B y 11B (visiÃ³n)
- [NVIDIA Jetson Documentation](https://docs.nvidia.com/jetson/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Llama 3.2 Vision Model](https://ollama.com/library/llama3.2-vision) - DocumentaciÃ³n del modelo multimodal

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ diseÃ±ado con fines educativos y de investigaciÃ³n. Consulte el archivo LICENSE para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- NVIDIA por el desarrollo del Jetson AGX Orin
- Comunidad de Ollama por el servidor de inferencia
- Contribuidores y testers del proyecto

## ğŸ“§ Contacto

Para preguntas, sugerencias o colaboraciones, por favor abra un issue en el repositorio.

---

**Nota**: Este proyecto estÃ¡ en desarrollo activo. Las mÃ©tricas y recomendaciones pueden actualizarse segÃºn nuevos hallazgos y optimizaciones.
